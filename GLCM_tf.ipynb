{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It should be a class the need to build it as a keras layer.\n",
    "class GLCM(Layer):\n",
    "    # batch_size is absolutely needed, and the data overlap should be hold by user.\n",
    "    def __init__(self, batch_size = 32, distance = 1, angle = 0, level = 2, symmetric = True, normed = True, **kwargs ):\n",
    "        super(GLCM, self).__init__(**kwargs)\n",
    "        self.distance = distance\n",
    "        self.angle = angle\n",
    "        self.level = level\n",
    "        self.symmetric = symmetric\n",
    "        self.normed = normed\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "    # all the inputs are assumed as 2D images like (batch, row, column, channel)\n",
    "    # The main function of this layer.\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        #cast the images into fixed channels and int32.\n",
    "        self.image = tf.cast(inputs/(3/self.level), dtype=tf.int32)\n",
    "        \n",
    "        #some necessary parameters\n",
    "        channel = inputs.shape[3].value\n",
    "        rows = self.image.shape[1].value\n",
    "        cols = self.image.shape[2].value\n",
    "        channel = self.image.shape[3].value\n",
    "\n",
    "        #calculate the subarray size of image \n",
    "        row = int(round(np.sin(self.angle))) * self.distance\n",
    "        col = int(round(np.cos(self.angle))) * self.distance\n",
    "\n",
    "        #We have two subarrays with same shape to express the two distinct entries\n",
    "        #We wil use Matrix operations instead of for-loop and if, for the ban on them from tensorflow. \n",
    "        if col > 0:\n",
    "            self.subarray_1 = self.image[:, :rows-row, :cols-col, :]\n",
    "            self.subarray_2 = self.image[:, row:, col:, :]\n",
    "        else:\n",
    "            self.subarray_1 = self.image[:, :self.rows-row, -col:, :]\n",
    "            self.subarray_2 = self.image[:, row:, :self.cols+col, :]\n",
    "         \n",
    "        # The sub_rows and sub_columns of our sub_arrays.\n",
    "        sub_row = self.subarray_1.shape[1].value\n",
    "        sub_column = self.subarray_1.shape[2].value\n",
    "    \n",
    "        # For tensorflow can only tile for dim<5, we will do like: \"tile->reshape\" most of time.\n",
    "        reshaped_subarray_1 = tf.reshape(self.subarray_1, shape=(self.batch_size, sub_row, sub_column, channel, 1))\n",
    "        reshaped_subarray_2 = tf.reshape(self.subarray_2, shape=(self.batch_size, sub_row, sub_column, channel, 1))\n",
    "        \n",
    "        tiled_subarray_1 = tf.tile(reshaped_subarray_1, [1, 1, 1, 1, self.level*self.level*(self.level-1)])\n",
    "        tiled_subarray_2 = tf.tile(reshaped_subarray_2, [1, 1, 1, 1, self.level*self.level*(self.level-1)])\n",
    "\n",
    "        reshape_tiled_subarray_1 = tf.reshape(tiled_subarray_1, shape=(self.batch_size, sub_row, sub_column, channel, self.level, self.level, self.level-1))\n",
    "        reshape_tiled_subarray_2 = tf.reshape(tiled_subarray_2, shape=(self.batch_size, sub_row, sub_column, channel, self.level, self.level, self.level-1))\n",
    "        \n",
    "        #This part we initial the constant sequence that will be used on computing.\n",
    "        #We do some operations to make it to match the shape we want.\n",
    "        sequence = tf.constant([])\n",
    "\n",
    "        for i in range(self.level):\n",
    "            hold = tf.ones(shape=(self.level-1))*i\n",
    "            sequence = tf.concat([sequence, hold], axis=0)\n",
    "\n",
    "        sequence = tf.cast(sequence, tf.int32)\n",
    "\n",
    "        sequence_reshape = tf.reshape(sequence, (self.level-1, self.level))\n",
    "        sequence_transpose = tf.transpose(sequence_reshape)\n",
    "        sequence_rev = tf.reverse(sequence_transpose, [0])\n",
    "        \n",
    "        sequence_rev_reshape = tf.reshape(sequence_rev, (1, 1, 1, 1, 1, self.level*(self.level-1)))\n",
    "        sequence_rev_reshape_tile = tf.tile(sequence_rev_reshape, [self.batch_size, sub_row, sub_column, channel, self.level, 1])\n",
    "        sequence_rev_reshaped_tile = tf.reshape(sequence_rev_reshape_tile, (self.batch_size, sub_row, sub_column, channel, self.level, self.level, self.level-1))\n",
    "\n",
    "        #Lagrange polynomials Denominator\n",
    "        GLCM_t = tf.tile(tf.reshape(sequence, (1, 1, 1, 1, self.level*(self.level-1))), [self.batch_size, sub_row, sub_column, channel, self.level])\n",
    "        GLCM_t_reshape = tf.reshape(GLCM_t, (self.batch_size, sub_row, sub_column, channel, self.level, self.level, self.level-1))\n",
    "        GLCM_Denominator_array = GLCM_t_reshape-sequence_rev_reshaped_tile\n",
    "        GLCM_Denominator = tf.reduce_prod(GLCM_Denominator_array, axis=6)\n",
    "        \n",
    "        #Lagrange polynomials Numerator\n",
    "        GLCM_Numerator_1_array = reshape_tiled_subarray_1-sequence_rev_reshaped_tile\n",
    "        GLCM_Numerator_2_array = reshape_tiled_subarray_2-sequence_rev_reshaped_tile\n",
    "\n",
    "        GLCM_Numerator_1 = tf.reduce_prod(GLCM_Numerator_1_array, axis=6)\n",
    "        GLCM_Numerator_2 = tf.reduce_prod(GLCM_Numerator_2_array, axis=6)\n",
    "        \n",
    "        # Subarrays generated after Lagrange polynomials.\n",
    "        GLCM_subarray_1 = GLCM_Numerator_1/GLCM_Denominator\n",
    "        GLCM_subarray_2 = GLCM_Numerator_2/GLCM_Denominator\n",
    "        \n",
    "        # Now, we need to do logic checking and merging on our subarrays to get the final GLCM.\n",
    "\n",
    "        # a and b = a*b\n",
    "        # We have to do transpose on GLCM_subarray_2 for its different location with GLCM_subarray_1 in GLCM. \n",
    "        GLCM_subarray_2_transpose = tf.transpose(GLCM_subarray_2, perm=[0, 1, 2, 3, 5, 4])\n",
    "        GLCM_single_entry = tf.multiply(GLCM_subarray_1, GLCM_subarray_2_transpose)\n",
    "        \n",
    "        # sum single entries to get the final GLCM.\n",
    "        GLCM = tf.reduce_sum(GLCM_single_entry, axis=[1, 2])\n",
    "\n",
    "        if self.symmetric:\n",
    "            GLCM = GLCM+tf.transpose(GLCM, perm=[0, 1, 3, 2])\n",
    "        if self.normed:\n",
    "            GLCM = GLCM/tf.reduce_sum(GLCM)\n",
    "        \n",
    "        #For some unknown casting by tensorflow function(?), we have to cast it back to float32 somehow.\n",
    "        #Same reason to reshape.\n",
    "        GLCM = tf.reshape(GLCM, (self.batch_size, channel*self.level*self.level))\n",
    "        GLCM = tf.cast(GLCM, tf.float32)\n",
    "        \n",
    "            \n",
    "        return GLCM\n",
    "    \n",
    "    #This function would be called in model compiling.\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #(batch_size, channel, GLCM_row, GLCM_column)\n",
    "        return (self.batch_size, input_shape[3]*self.level*self.level)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
